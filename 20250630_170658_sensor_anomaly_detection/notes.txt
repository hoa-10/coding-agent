# Title: Sensor Anomaly Detection Using Window-Based Classification
# Experiment description: Train a classifier (e.g., Random Forest, Logistic Regression, or LSTM) on time-windowed sensor data to detect anomalies. The dataset is split into overlapping windows to preserve temporal patterns. The goal is to predict whether a window of sensor readings contains an anomaly using classical machine learning (e.g., scikit-learn) or deep learning (e.g., LSTM). Models are evaluated using validation accuracy, precision, recall, and F1-score.
## Run 0: Baseline
Results: {'val_accuracy': 0.8976744186046511, 'val_precision': 0.9387755102040817, 'val_recall': 0.8518518518518519, 'val_f1': 0.8932038834951457, 'test_accuracy': 0.8697674418604651, 'test_precision': 0.8631578947368421, 'test_recall': 0.845360824742268, 'test_f1': 0.8541666666666666, 'pipeline_parameters': {'window_size': 10, 'stride': 1, 'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15}, 'model_parameters': {'model_name': 'RandomForestClassifier', 'n_estimators': 100, 'random_state': 42}}
Description: Baseline results.

## Run 1
Results: {'val_accuracy': 0.958139534883721, 'val_precision': 1.0, 'val_recall': 0.6896551724137931, 'val_f1': 0.8163265306122449, 'test_accuracy': 0.9441860465116279, 'test_precision': 0.9444444444444444, 'test_recall': 0.6071428571428571, 'test_f1': 0.7391304347826086, 'pipeline_parameters': {'window_size': 10, 'stride': 1, 'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15}, 'model_parameters': {'model_name': 'RandomForestClassifier', 'n_estimators': 200, 'random_state': 42}}
Description: This run increased the `n_estimators` hyperparameter for the RandomForestClassifier from 100 to 200. The aim was to observe the impact of a more complex model on anomaly detection performance. The results indicate an improvement in overall accuracy and precision, but a noticeable reduction in recall, suggesting the model became more selective in identifying anomalies.


## Run 2
Results: {'val_accuracy': 0.9107981220657277, 'val_precision': 1.0, 'val_recall': 0.3448275862068966, 'val_f1': 0.5128205128205128, 'test_accuracy': 0.9158878504672897, 'test_precision': 0.9166666666666666, 'test_recall': 0.39285714285714285, 'test_f1': 0.55, 'pipeline_parameters': {'window_size': 20, 'stride': 1, 'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15}, 'model_parameters': {'model_name': 'RandomForestClassifier', 'n_estimators': 100, 'random_state': 42}}
Description: This run increased the `WINDOW_SIZE` from 10 to 20 while keeping `n_estimators` at its baseline of 100. The objective was to see how a larger temporal window affects anomaly detection. The results show a significant drop in recall, indicating that the model is missing a large proportion of actual anomalies, even though its precision remains high. This suggests that a larger window size, in isolation, does not necessarily improve the model's ability to identify anomalies comprehensively, and might make it overly cautious or introduce more noise.

## Run 3
Results: {'val_accuracy': 0.8604651162790697, 'val_precision': 1.0, 'val_recall': 0.14285714285714285, 'val_f1': 0.25, 'test_accuracy': 0.8636363636363636, 'test_precision': 0.0, 'test_recall': 0.0, 'test_f1': 0.0, 'pipeline_parameters': {'window_size': 10, 'stride': 5, 'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15}, 'model_parameters': {'model_name': 'RandomForestClassifier', 'n_estimators': 100, 'random_state': 42}}
Description: This run increased the `STRIDE` from 1 to 5, keeping `WINDOW_SIZE` at 10 and `n_estimators` at 100. The objective was to see how reducing window overlap impacts performance. The results show a severe degradation, particularly in recall and F1-score on both validation and test sets, with test recall dropping to 0.0. This indicates that a larger stride leads to the model completely failing to detect anomalies, likely due to significant loss of temporal context or insufficient data points when windows are less overlapping.

## Run 4
Results: {'val_accuracy': 0.8883720930232558, 'val_precision': 1.0, 'val_recall': 0.1724137931034483, 'val_f1': 0.29411764705882354, 'test_accuracy': 0.8697674418604651, 'test_precision': 0.0, 'test_recall': 0.0, 'test_f1': 0.0, 'pipeline_parameters': {'window_size': 10, 'stride': 1, 'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15}, 'model_parameters': {'model_name': 'LogisticRegression', 'solver': 'liblinear', 'random_state': 42}}
Description: This run switched the classification model to `LogisticRegression` while maintaining baseline pipeline parameters. The intention was to assess the performance of a simpler, linear model. The results indicate very poor anomaly detection capabilities, with extremely low recall and F1-scores (zero on the test set). This suggests that `LogisticRegression` is not suitable for this anomaly detection task given the current data and feature representation.

## Run 5
Description: This run combines increased `n_estimators` (200) for the `RandomForestClassifier` and an increased `WINDOW_SIZE` (20). This aims to explore if a more complex Random Forest model can leverage a larger temporal context to improve overall anomaly detection, potentially addressing the low recall observed in previous runs while maintaining good precision.

---
# Plots

## Plot 1: Metrics on Validation Set
**Filename:** `plots/metrics_val_set.png`
**Description:** This plot presents a bar chart comparing the performance of different experiment runs on the validation set across four key metrics: Accuracy, Precision, Recall, and F1-score. Each bar group represents a different run (Baseline, RF n_est=200, RF WS=20, RF S=5, Logistic Regression, RF n_est=200 WS=20). The individual bars within each group show the specific value for Accuracy, Precision, Recall, and F1-score for that run. This visualisation allows for a quick comparison of how each configuration affected the model's ability to classify anomalies on unseen data during validation, highlighting trade-offs between precision (correctly identifying positive anomalies) and recall (identifying all actual anomalies). Notably, some runs might show very high precision but very low recall, indicating a highly conservative model that rarely predicts anomalies but is correct when it does.

## Plot 2: Metrics on Test Set
**Filename:** `plots/metrics_test_set.png`
**Description:** This plot mirrors the structure of the validation set plot but displays the performance metrics (Accuracy, Precision, Recall, F1-score) for each experiment run on the final, completely unseen test set. This provides a crucial assessment of the models' generalisation capabilities. By comparing these results to the validation set, one can infer potential overfitting or the true robustness of the model configurations in real-world scenarios. Similar to the validation plot, it's expected to see varied performance, particularly in recall, where some models might struggle to detect anomalies in a production-like environment if the patterns are too subtle or the model is over-optimised for precision.

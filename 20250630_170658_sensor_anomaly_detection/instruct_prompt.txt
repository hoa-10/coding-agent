Generate a Python script named `run_pipeline.py` that implements a machine learning pipeline for anomaly detection on time-series sensor data.

**Task Overview:**
The goal is to train a classifier to predict whether a sensor reading window indicates an anomaly, optimizing for validation accuracy. The dataset `sensor_data.csv` contains numerical sensor readings over time.

**Dataset Information (`sensor_data.csv`):**
- **Columns:** `sensor_1`, `sensor_2`, `sensor_3`.
- **Data Type:** All columns are numerical (float).
- **Structure:** 1440 rows x 3 columns, representing time-ordered sensor readings.
- **Missing Values:** Assume no missing values for this task based on the provided snippet.
- **Temporal Patterns:** The data is sequential, and temporal patterns are crucial for anomaly detection.

**Pipeline Steps:**

1.  **Data Loading:**
    *   Load the `sensor_data.csv` file using pandas.

2.  **Synthetic Anomaly Label Generation:**
    *   Since the input `sensor_data.csv` does not contain anomaly labels, generate synthetic labels to enable supervised classification.
    *   Use an unsupervised anomaly detection method (e.g., `IsolationForest` from `sklearn.ensemble`) on the raw sensor data to identify initial "anomalies".
    *   Fit `IsolationForest` on the entire dataset.
    *   Predict outliers: `IsolationForest` typically outputs -1 for outliers/anomalies and 1 for inliers/normal data.
    *   Map these predictions to binary labels: 1 for anomaly (-1 from IsolationForest) and 0 for normal (1 from IsolationForest). This will serve as the target variable `y` for supervised classification.

3.  **Time-Windowing Preprocessing:**
    *   Implement a function to transform the sequential sensor data into fixed-size windows.
    *   Define a `window_size` (e.g., 10) and `stride` (e.g., 1).
    *   Each window will become a single sample for the classifier. For a `window_size` of `W` and `F` features, each new sample will have `W * F` features.
    *   The `y` label for each window should correspond to the anomaly label of the *last* point in that window.

4.  **Feature Scaling:**
    *   Apply `StandardScaler` from `sklearn.preprocessing` to the windowed features (`X`). Fit the scaler on the training data and transform all splits.

5.  **Data Splitting:**
    *   Split the windowed dataset into training, validation, and test sets, preserving the temporal order.
    *   Use a split ratio of approximately 70% for training, 15% for validation, and 15% for testing.

6.  **Model Selection and Configuration:**
    *   Choose `RandomForestClassifier` from `sklearn.ensemble` as the primary classification model due to its robustness and adherence to the "scikit-learn" constraint, avoiding libraries that are rarely installed.
    *   Set reasonable hyperparameters for `RandomForestClassifier`, such as `n_estimators=100`, `random_state=42`.

7.  **Model Training:**
    *   Train the `RandomForestClassifier` model on the training data.

8.  **Prediction and Evaluation:**
    *   Make predictions on both the validation set and the test set.
    *   Compute the following evaluation metrics on both validation and test sets:
        *   Accuracy
        *   Precision
        *   Recall
        *   F1-score
    *   Ensure all metrics are calculated for the positive class (anomaly, which is 1).

9.  **Results Saving:**
    *   Create a directory named `result` if it does not already exist.
    *   Save a JSON file named `results.json` inside the `result` directory.
    *   The `results.json` file must contain:
        *   All computed evaluation metrics for both validation and test sets (e.g., `val_accuracy`, `test_precision`, etc.).
        *   The hyperparameters used for the `RandomForestClassifier` (e.g., `model_name`, `n_estimators`, `random_state`).
        *   The `window_size` and `stride` used for time-windowing.

**Additional Requirements:**
*   The script should be modular, using functions for different pipeline stages (e.g., `create_windows`, `train_model`, `evaluate_model`).
*   The script must be executable directly.
*   Do not include any comments or notes in the generated Python source code.
*   The script should only use standard libraries (e.g., `pandas`, `numpy`, `sklearn`, `json`, `os`).

**Output File Name:** `run_pipeline.py`
```
Generate a Python script named `run_pipeline.py` that implements a deep learning pipeline for NDT-Sensor Anomaly Detection. The script must classify between two classes: Anomaly (0) and Normal (1) using the provided dataset. It should include data loading, preprocessing, model definition, training, evaluation, and result saving.

**Dataset Information:**
The dataset is located at `pect_ndt_full_dataset.npz`. You *must* use the following exact code snippet to load the data:
```python
import numpy as np
import os
import sys

# Check if the dataset file exists
dataset_path = 'pect_ndt_full_dataset.npz'
if not os.path.exists(dataset_path):
    print(f"Error: Dataset file not found at {dataset_path}")
    sys.exit(1)

data = np.load(dataset_path)
X_train = data['X_train']
y_train = data['y_train']
X_valid = data['X_valid']
y_valid = data['y_valid']
X_scan = data['X_scan']
X_in_corr = data['X_in_corr']
Xc = data['Xc']
Xg = data['Xg']
m = data['m']
st = data['st']
```

**Pipeline Steps:**

1.  **Data Preparation and Splitting:**
    *   Combine `X_train` and `X_valid` into a single feature array (`X_combined`).
    *   Combine `y_train` and `y_valid` into a single target array (`y_combined`).
    *   Perform a stratified split of `X_combined` and `y_combined` into training, validation, and test sets with an 80% train, 10% validation, and 10% test ratio. Use `random_state=42` for reproducibility.
    *   Apply `StandardScaler` to the features. The scaler must be fitted *only* on the training data (`X_train_scaled`) and then used to transform `X_train_scaled`, `X_val_scaled`, and `X_test_scaled`.

2.  **Model Configuration:**
    *   Define a deep learning model using TensorFlow/Keras.
    *   The model should be a Sequential API with the following structure:
        *   An `InputLayer` matching the number of features of the scaled training data.
        *   At least two `Dense` hidden layers with 'relu' activation (e.g., 64 and 32 units).
        *   A `Dense` output layer with 1 unit and 'sigmoid' activation for binary classification.
    *   Compile the model with the 'adam' optimizer and 'binary_crossentropy' loss.
    *   Set `tf.random.set_seed(42)` and `np.random.seed(42)` at the beginning of the script for full reproducibility.

3.  **Training Process:**
    *   Train the model using the training data (`X_train_scaled`, `y_train`).
    *   Use the validation data (`X_val_scaled`, `y_val`) during training (via the `validation_data` argument in `model.fit`).
    *   Train for a reasonable number of epochs (e.g., 50).
    *   Implement `EarlyStopping` (from `keras.callbacks`) to monitor validation loss with a patience of 5 epochs to prevent overfitting.

4.  **Evaluation:**
    *   Evaluate the trained model on the `test` set (`X_test_scaled`, `y_test`).
    *   Calculate and print the following metrics:
        *   Accuracy
        *   Precision (macro average)
        *   Recall (macro average)
        *   F1-score (macro average)
        *   ROC AUC score (for positive class prediction probabilities)
    *   Ensure all metrics are imported from `sklearn.metrics`.

5.  **Save Results:**
    *   Create a directory named `result` in the current working directory if it does not already exist.
    *   Save a JSON file named `results.json` inside the `result` directory.
    *   This JSON file must contain:
        *   All computed evaluation metrics (accuracy, precision, recall, f1-score, roc_auc).
        *   The hyperparameters used for the model (e.g., number of epochs, batch size, optimizer, layer units, activation functions, `random_state`).

**Important Requirements and Constraints:**

*   **No Dummy Data:** Do NOT generate any dummy, fake, or random data. Work only with the actual data loaded from `pect_ndt_full_dataset.npz`.
*   **Reproducibility:** Ensure the entire pipeline is reproducible by setting `random_state` for data splitting and seeds for TensorFlow/NumPy.
*   **Library Restriction:** Limit library usage to commonly available and easily installable ones (e.g., `numpy`, `sklearn`, `tensorflow`/`keras`, `json`, `os`, `sys`). Avoid rare or hard-to-install packages.
*   **Script Name:** The generated script must be named `run_pipeline.py`.
*   **Code Clarity:** The generated Python script should be clear, concise, and modular. Do not include any comments or extraneous notes within the *generated Python code itself*.

```python
# The instruct LLM will fill this section with the generated Python script.
```
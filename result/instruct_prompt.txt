```
You are an expert Python programmer specializing in machine learning pipelines. Your task is to write a high-quality Python script named `run_pipeline.py` that implements an advanced deep learning pipeline for Non-Destructive Testing (NDT) data analysis.

The script must perform data loading, preprocessing, deep learning model training, evaluation, and results saving.

---

**Dataset Details:**

The dataset is located at `C:\Users\user\Desktop\coding-agent\pect_ndt_full_dataset.npz`. This is a real dataset and must be used directly. Do NOT create or use any dummy, fake, or synthetic data. Do NOT generate random data. If the file does not exist, the script should print an error message and exit gracefully.

The dataset can be loaded using the following specific code:

```python
import numpy as np
import os

# Check if the file exists before attempting to load
dataset_path = 'pect_ndt_full_dataset.npz'
if not os.path.exists(dataset_path):
    print(f"Error: The dataset file '{dataset_path}' was not found.")
    print("Please ensure the file is in the correct directory.")
    exit()

try:
    data = np.load(dataset_path)
    X_train = data['X_train']
    y_train = data['y_train']
    X_valid = data['X_valid']
    y_valid = data['y_valid']
    # Other potential keys are X_scan, X_in_corr, Xc, Xg, m, st.
    # Focus on X_train, y_train, X_valid, y_valid for the supervised task.
except KeyError as e:
    print(f"Error: Missing expected key in the dataset file: {e}.")
    print("Please ensure 'X_train', 'y_train', 'X_valid', 'y_valid' keys are present.")
    exit()
except Exception as e:
    print(f"An unexpected error occurred while loading the dataset: {e}")
    exit()
```

The primary task, inferred from "Advanced NDT Signal Analysis with Deep Learning" and "anomaly detection, and predictive modeling" with provided `y_train`/`y_valid`, is **binary classification** (e.g., classifying normal vs. anomalous NDT signals). The script should initially assume `y_train` and `y_valid` contain binary labels (0 or 1). If upon runtime inspection of `np.unique(y_train)` or `y_train.dtype` it becomes clear the task is multi-class classification or regression, adapt the model and metrics accordingly, but prioritize binary classification as the initial setup.

---

**Pipeline Requirements for `run_pipeline.py`:**

1.  **Imports**: Use only commonly available and easy-to-install libraries (e.g., `numpy`, `sklearn`, `tensorflow`/`keras`, `json`, `os`). Avoid obscure or rarely used libraries.
2.  **Data Loading**: Implement the exact data loading code provided above. Ensure the script handles `FileNotFoundError` and `KeyError` gracefully.
3.  **Preprocessing**:
    *   **Feature Scaling**: Apply `sklearn.preprocessing.StandardScaler` to `X_train` and `X_valid`. Fit the scaler on `X_train` and transform both `X_train` and `X_valid`.
    *   **Target Preprocessing (if classification)**:
        *   Inspect `y_train` to determine if it's binary, multi-class, or regression.
        *   If the task is determined to be binary classification (two unique integer values in `y_train`), ensure `y_train` and `y_valid` are suitable for binary cross-entropy (e.g., integers 0/1, or floats 0.0/1.0).
        *   If the unique values in `y_train` suggest multi-class classification (more than two unique integer values), convert `y_train` and `y_valid` to one-hot encoding using `tf.keras.utils.to_categorical`.
        *   If `y_train` appears continuous (e.g., float type with many unique values), adapt the model and metrics for a regression task.
4.  **Deep Learning Model Configuration**:
    *   Use `tf.keras.Sequential` to build a deep learning model.
    *   The input shape of the first layer should be `(X_train.shape[1],)` if `X_train` is 2D (samples, features). If `X_train` is multi-dimensional (e.g., 3D for sequences, 4D for images), use `X_train.shape[1:]` and consider appropriate layers like `Conv1D`, `Conv2D`, or `Flatten` as the first layer. For general NDT signals, a simple `Dense` network with `Flatten` is a good starting point if input is higher than 2D.
    *   **Architecture (Suggestion for Binary Classification - adapt for others)**:
        *   Start with a `Flatten` layer if `X_train` has more than 2 dimensions.
        *   One or two `Dense` layers with `relu` activation (e.g., 128 units, then 64 units).
        *   An output `Dense` layer:
            *   For binary classification: 1 unit with `sigmoid` activation.
            *   For multi-class classification: number of classes units with `softmax` activation.
            *   For regression: 1 unit with linear activation.
    *   **Compilation**:
        *   Optimizer: `adam` (with default learning rate).
        *   Loss function:
            *   For binary classification: `binary_crossentropy`.
            *   For multi-class classification: `categorical_crossentropy`.
            *   For regression: `mse` (Mean Squared Error).
        *   Metrics: `accuracy` (for classification), `mae` (for regression).
    *   **Hyperparameters**:
        *   `epochs`: 30
        *   `batch_size`: 32
5.  **Training Process**:
    *   Train the model using `model.fit()`.
    *   Use `X_train`, `y_train` for training data.
    *   Use `validation_data=(X_valid, y_valid)` for evaluation during training.
    *   Do not use random seeds for data splitting or shuffling, as the data splits (`X_train`, `y_train`, `X_valid`, `y_valid`) are already pre-defined and fixed in the `.npz` file.
6.  **Evaluation**:
    *   Evaluate the trained model on the `X_valid`, `y_valid` dataset (treating it as the final evaluation set since no explicit `X_test`/`y_test` are provided).
    *   **For Binary Classification (primary task)**: Compute and store the following metrics using `sklearn.metrics` where appropriate, in addition to `model.evaluate` results:
        *   Loss (from `model.evaluate`)
        *   Accuracy (from `model.evaluate` and `sklearn.metrics.accuracy_score`)
        *   Precision (using `sklearn.metrics.precision_score`)
        *   Recall (using `sklearn.metrics.recall_score`)
        *   F1-Score (using `sklearn.metrics.f1_score`)
        *   ROC AUC Score (using `sklearn.metrics.roc_auc_score`)
        *   Predictions should be obtained from `model.predict(X_valid)` and binarized at a threshold (e.g., 0.5) for classification metrics.
    *   **For Multi-class Classification (if applicable)**: Adapt metrics (e.g., macro/weighted averages for precision, recall, f1, use `roc_auc_score` with `multi_class='ovr'` or `multi_class='ovo'` if applicable).
    *   **For Regression (if applicable)**: Compute Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2 Score).
7.  **Save Results**:
    *   Create a directory named `result` in the current working directory if it does not already exist (using `os.makedirs`).
    *   Save the following information to a JSON file named `results.json` within the `result` directory:
        *   All computed evaluation metrics on the validation set.
        *   The hyperparameters used for the model (e.g., epochs, batch_size, model architecture summary/description).
        *   The JSON should be well-formatted and readable.

---

**General Instructions for the Script:**

*   The script should be modular, clean, and well-structured.
*   Do NOT include any notes or comments in the generated source code.
*   Ensure that the script does NOT generate or use any random data or random seeds for data manipulation. The provided data from `pect_ndt_full_dataset.npz` is the only data source.
*   The only randomness allowed should be for the initial weights of the neural network model, which is handled internally by TensorFlow/Keras.
*   The script should print clear messages indicating progress (e.g., "Loading data...", "Training model...", "Evaluating model...", "Saving results...").

```python
# The content of run_pipeline.py should be generated here.
```
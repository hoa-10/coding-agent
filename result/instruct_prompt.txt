**Prompt for Instruct LLM:**

You are an expert Python programmer specializing in machine learning and deep learning. Your task is to write a high-quality Python script named `run_pipeline.py` that implements an advanced deep learning pipeline for Non-Destructive Testing (NDT) signal analysis. The objective is to perform anomaly detection and predictive modeling on the provided NDT data, encompassing data preprocessing, deep learning model training, comprehensive evaluation, and result visualization.

The script must adhere to the following strict requirements:

1.  **Reproducibility**: Set a global random seed (e.g., `42`) for NumPy and TensorFlow/Keras operations to ensure reproducible results. Do not generate any random, synthetic, or dummy data; work exclusively with the actual data loaded from the specified file.

2.  **Dataset Loading**:
    *   Load the dataset from the file path: `pect_ndt_full_dataset.npz`.
    *   The file contains the following arrays: `X_train`, `y_train`, `X_valid`, `y_valid`, `X_scan`, `X_in_corr`, `Xc`, `Xg`, `m`, `st`.
    *   Use the exact provided Python code for data loading:
        ```python
        import numpy as np

        data = np.load('pect_ndt_full_dataset.npz')
        X_train = data['X_train']
        y_train = data['y_train']
        X_valid = data['X_valid']
        y_valid = data['y_valid']
        X_scan = data['X_scan']
        X_in_corr = data['X_in_corr']
        Xc = data['Xc']
        Xg = data['Xg']
        m = data['m']
        st = data['st']
        ```
    *   Implement robust error handling: If the file `pect_ndt_full_dataset.npz` is not found, print the error message "Error: Dataset file 'pect_ndt_full_dataset.npz' not found. Please ensure the file is in the correct directory." to the console and then terminate the script gracefully (e.g., using `sys.exit(1)`).

3.  **Data Preprocessing**:
    *   **Feature Scaling**: Apply `StandardScaler` from `sklearn.preprocessing` to the `X_train` and `X_valid` datasets. The scaler must be `fit` *only* on `X_train` and then used to `transform` both `X_train` and `X_valid`.
    *   **Label Preparation**: Ensure that `y_train` and `y_valid` are in the appropriate numerical format (e.g., integers `0` or `1`) suitable for binary classification.

4.  **Model Configuration (Deep Learning)**:
    *   Construct a deep learning model using `tensorflow.keras.models.Sequential`.
    *   The model should be designed for binary classification, inferring the input shape from the preprocessed `X_train`.
    *   **Recommended Architecture**:
        *   An `InputLayer` that matches the number of features in `X_train_scaled` (i.e., `X_train_scaled.shape[1]`).
        *   At least two `Dense` hidden layers with `relu` activation (e.g., 64 units, 32 units).
        *   A final `Dense` output layer with `1` unit and `sigmoid` activation for binary classification.
    *   Compile the model using the `adam` optimizer (with a learning rate of `0.001`), `binary_crossentropy` as the loss function, and `accuracy` as a metric.

5.  **Training Process**:
    *   Train the configured deep learning model using the preprocessed `X_train_scaled` and `y_train`.
    *   Utilize `X_valid_scaled` and `y_valid` as the validation set during training to monitor performance.
    *   Set the training parameters: `epochs` to `30` and `batch_size` to `32`.

6.  **Evaluation**:
    *   After the training completes, evaluate the model's performance on the `X_valid_scaled` dataset (which serves as the final test set for this pipeline).
    *   Compute and display the following standard classification metrics from `sklearn.metrics`:
        *   Accuracy Score
        *   Precision Score
        *   Recall Score
        *   F1 Score
        *   ROC AUC Score (using predicted probabilities)
    *   Print these evaluation metrics clearly to the console.

7.  **Visualization**:
    *   Generate a plot that displays the training and validation loss curves over epochs.
    *   On the same plot, also display the training and validation accuracy curves over epochs.
    *   Save this plot as `training_history.png` within the designated `result` directory.

8.  **Save Results**:
    *   Create a directory named `result` in the current working directory if it does not already exist (use `os.makedirs`).
    *   Save all computed evaluation metrics and the model's hyperparameters into a JSON file named `results.json` inside the `result` directory.
    *   The `results.json` file must contain:
        *   All numerical evaluation metrics (accuracy, precision, recall, f1, roc_auc).
        *   Key hyperparameters used for training and the model (e.g., `epochs`, `batch_size`, `learning_rate`, `random_seed`).
        *   A brief string describing the model architecture (e.g., "Sequential Multi-Layer Perceptron for binary classification").

9.  **Code Structure and Libraries**:
    *   The Python script should be modular, well-structured, and easy to read.
    *   Do NOT include any notes or comments within the generated Python source code.
    *   Limit the usage of external libraries to commonly used ones for machine learning, such as `numpy`, `sklearn`, `tensorflow`, `matplotlib`, `json`, `os`, and `sys`. Avoid obscure or rarely installed libraries.
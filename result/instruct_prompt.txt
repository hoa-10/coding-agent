Generate a Python script named `run_pipeline.py` that implements a deep learning pipeline for Non-Destructive Testing (NDT) signal classification. The script's objective is to classify time-series electromagnetic signals as 'good' (0) or 'defect' (1), addressing the significant class imbalance present in the dataset.

**Dataset Details:**
The dataset is stored in a NumPy `.npz` file named `pect_ndt_full_dataset.npz`. The script must load the following arrays from this file:
- `X_train`: Training signals (shape: `[15456, 500, 1]`, dtype: `float64`). Each signal is a 1D waveform of 500 time points.
- `y_train`: Training labels (shape: `[15456]`, dtype: `int32`; `0` for good, `1` for defect).
- `X_valid`: Validation signals (shape: `[10304, 500, 1]`, dtype: `float64`).
- `y_valid`: Validation labels (shape: `[10304]`, dtype: `int32`).
- `m`: Mean values for signal normalization (shape: `[1, 500, 1]`).
- `st`: Standard deviation values for signal normalization (shape: `[1, 500, 1]`).

**Required Code for Data Loading:**
```python
import numpy as np
import os
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import json

# Set a fixed random seed for reproducibility of model initialization and training
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

try:
    data = np.load('pect_ndt_full_dataset.npz')
    X_train = data['X_train']
    y_train = data['y_train']
    X_valid = data['X_valid']
    y_valid = data['y_valid']
    m = data['m']
    st = data['st']
except FileNotFoundError:
    print("Error: 'pect_ndt_full_dataset.npz' not found. Please ensure the dataset file is in the same directory.")
    exit()
except KeyError as e:
    print(f"Error: Missing expected key in the dataset file: {e}. Please check the dataset integrity.")
    exit()
```

**Pipeline Steps:**

1.  **Preprocessing:**
    *   Apply Z-score normalization to `X_train` and `X_valid` using the provided `m` (mean) and `st` (standard deviation) arrays. The formula for normalization is `(X - m) / st`.
    *   The signals are already in the correct 3D shape `(samples, time_points, features)` for 1D CNNs, so no further reshaping is required.
    *   Ensure `y_train` and `y_valid` are suitable for binary classification (e.g., `int` type).

2.  **Model Architecture (1D Convolutional Neural Network):**
    *   Construct a TensorFlow/Keras 1D CNN model for binary classification of time series signals.
    *   **Input Layer:** `Input(shape=(500, 1))` to match the signal dimensions.
    *   **Convolutional Blocks:** Stack multiple blocks consisting of `Conv1D` layers (e.g., 32 to 128 filters, kernel size 3 or 5, `relu` activation), followed by `MaxPooling1D` layers (e.g., pool size 2) for feature extraction and downsampling.
    *   **Flatten Layer:** After convolutional blocks, use a `Flatten` layer to convert the 3D output to 1D for dense layers.
    *   **Dense Layers:** Add one or more `Dense` layers with `relu` activation (e.g., 64 or 32 units). Include `Dropout` layers (e.g., 0.5) for regularization.
    *   **Output Layer:** A single `Dense` layer with `sigmoid` activation (`Dense(1, activation='sigmoid')`) for binary classification.

3.  **Model Compilation:**
    *   **Optimizer:** `Adam`.
    *   **Loss Function:** `BinaryCrossentropy`.
    *   **Metrics:** Compile the model with `accuracy`, `Precision(name='precision')`, `Recall(name='recall')`, and `AUC(name='auc')`.

4.  **Addressing Class Imbalance:**
    *   Calculate class weights for `y_train` based on the inverse of class frequencies. Pass these calculated weights to the `model.fit()` method using the `class_weight` argument. This helps the model pay more attention to the minority 'defect' class.

5.  **Training Process:**
    *   Train the CNN model using the preprocessed `X_train` and `y_train`.
    *   Validate the model performance during training using `X_valid` and `y_valid`.
    *   **Hyperparameters:**
        *   `epochs`: 50
        *   `batch_size`: 64
    *   **Callbacks:**
        *   `EarlyStopping`: Monitor `val_loss`, `patience=10`, `restore_best_weights=True`.
        *   `ReduceLROnPlateau`: Monitor `val_loss`, `factor=0.5`, `patience=5`, `min_lr=0.00001`.

6.  **Evaluation:**
    *   After training, evaluate the model on the `X_valid` and `y_valid` datasets to obtain final performance metrics.
    *   Compute and print the following metrics:
        *   Accuracy
        *   Precision (for class 1: defect)
        *   Recall (for class 1: defect)
        *   F1-Score (for class 1: defect)
        *   ROC-AUC
    *   **Confusion Matrix:** Generate and save a confusion matrix plot for the validation set. The plot should clearly label axes and title. Save it as `confusion_matrix.png` in the `results` directory.
    *   **ROC Curve:** Generate and save an ROC curve plot for the validation set, showing the AUC score. Save it as `roc_curve.png` in the `results` directory.

7.  **Save Results:**
    *   Create a directory named `results` if it does not exist using `os.makedirs`.
    *   Save a JSON file named `results.json` inside the `results` directory. This file must contain:
        *   All computed evaluation metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC).
        *   The hyperparameters used for training (epochs, batch_size, RANDOM_SEED, early stopping patience, learning rate reduction factor/patience, optimizer name).
        *   A concise description of the model architecture (e.g., "1D CNN with 3 Conv1D layers (32, 64, 128 filters), 2 Dense layers (64, 32 units)").

The script must be modular, adaptable, and solely use the provided data, without generating any random or synthetic data. Do not include notes or comments in the generated source code.
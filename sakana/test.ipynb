{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gán API key (nên lấy từ biến môi trường)\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Đặt API key vào biến môi trường GEMINI_API_KEY\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import get_aggregator_system_msg\n",
    "AGGREGATOR_SYSTEM_MSG = get_aggregator_system_msg(\"../sensor_data.csv\", \"../idea.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "You are a highly skilled Python programmer and Machine Learning Engineer. Your task is to implement a complete time-series forecasting pipeline in a single Python script named `run_pipeline.py`. Follow the detailed instructions below precisely.\n",
      "\n",
      "**Overall Goal:** Predict the value of SensorA 10 minutes into the future, using a 10-minute historical window of readings from all three sensors (SensorA, SensorB, SensorC). This is a univariate time-series forecasting (regression) problem.\n",
      "\n",
      "**Dataset:**\n",
      "*   File: `../sensor_data.csv`\n",
      "*   Content: Minute-level readings (1,440 rows) for 'SensorA', 'SensorB', 'SensorC'.\n",
      "\n",
      "**Model Choice:**\n",
      "*   **Long Short-Term Memory (LSTM) Neural Network**. LSTMs are chosen due to their inherent ability to capture temporal dependencies and patterns in sequential data, making them well-suited for forecasting problems involving \"sensor drift\" and leveraging historical windows.\n",
      "\n",
      "**Detailed Implementation Steps:**\n",
      "\n",
      "1.  **Setup and Data Loading:**\n",
      "    *   Import necessary libraries: `pandas`, `numpy`, `sklearn.preprocessing`, `sklearn.metrics`, `tensorflow.keras.models`, `tensorflow.keras.layers`, `tensorflow.keras.callbacks`, `matplotlib.pyplot`, `json`, `os`.\n",
      "    *   Load the `../sensor_data.csv` file into a pandas DataFrame.\n",
      "    *   The 'time' column is descriptive and not a feature; ensure sensor columns are treated as numeric.\n",
      "\n",
      "2.  **Data Preprocessing:**\n",
      "    *   **Handle Missing Values:**\n",
      "        *   Convert any non-numeric entries (e.g., empty strings) in 'SensorA', 'SensorB', 'SensorC' columns to `np.nan`.\n",
      "        *   Apply **linear interpolation** (`df.interpolate(method='linear', limit_direction='both', axis=0)`) to fill all `np.nan` values across the sensor columns. This method is appropriate for continuous time-series data.\n",
      "    *   **Feature Scaling:**\n",
      "        *   Initialize a `StandardScaler`.\n",
      "        *   **Important:** The scaler will be fitted *only on the training data's sensor columns* later, but prepare for its use.\n",
      "\n",
      "3.  **Data Preparation for LSTM (Windowing):**\n",
      "    *   Define `LOOK_BACK_WINDOW = 10` (minutes) and `FORECAST_HORIZON = 10` (minutes).\n",
      "    *   Create input sequences (`X`) and corresponding target values (`y`) using a sliding window approach:\n",
      "        *   Each `X` sample will be a 3D NumPy array of shape `(LOOK_BACK_WINDOW, num_features)`, representing `num_features=3` sensor readings over 10 minutes. For a given time `t`, the input window will span `[t-9, ..., t]`.\n",
      "        *   Each `y` target will be the single `SensorA` value at `t + FORECAST_HORIZON`. Specifically, if an input window ends at time `t`, the target `y` will be `SensorA`'s value at `t+10`.\n",
      "        *   The total number of samples generated will be `len(df) - LOOK_BACK_WINDOW - FORECAST_HORIZON + 1`.\n",
      "\n",
      "4.  **Train/Test Split:**\n",
      "    *   Perform a **chronological split** of the prepared `X` and `y` data.\n",
      "    *   Allocate 80% of the *earliest* samples for training and the remaining 20% for testing.\n",
      "    *   Example: `train_size = int(len(X) * 0.8)`. `X_train, y_train = X[:train_size], y[:train_size]`. `X_test, y_test = X[train_size:], y[train_size:]`.\n",
      "\n",
      "5.  **Scaling Training and Test Data:**\n",
      "    *   Reshape `X_train` and `X_test` for `StandardScaler` (e.g., `X_train.reshape(-1, num_features)`).\n",
      "    *   Fit the `StandardScaler` *only* on the reshaped `X_train` data (all 3 sensor columns).\n",
      "    *   Transform both the reshaped `X_train` and `X_test` using the fitted scaler.\n",
      "    *   Reshape the scaled `X_train` and `X_test` back to their 3D LSTM input shape `(num_samples, LOOK_BACK_WINDOW, num_features)`.\n",
      "    *   Scale `y_train` and `y_test` separately using the *same scaler* (only fitting on the SensorA part of `X_train` is complex; simplest is to fit the scaler on all sensor data before splitting, but the prompt's `model_restriction` section hints at fitting on train data. Let's adjust: scale *all* sensor data first, then split into X, y, train/test. This simplifies inverse transforming).\n",
      "\n",
      "    **Revised Scaling Strategy:**\n",
      "    *   After missing value imputation, apply `StandardScaler` to the *entire* 'SensorA', 'SensorB', 'SensorC' DataFrame (`df_sensors`).\n",
      "    *   Fit the `StandardScaler` instance on `df_sensors`. This scaler will then be used for inverse transformation.\n",
      "    *   Then, proceed with the windowing (`X`, `y`) and train/test split on this already scaled data.\n",
      "    *   This ensures `y_train` and `y_test` are also correctly scaled by the same scaler that processed the input features, which is crucial for consistent inverse transformation later.\n",
      "\n",
      "6.  **Model Building (LSTM):**\n",
      "    *   Use `tensorflow.keras.Sequential` to build the model.\n",
      "    *   **Input Layer:** `LSTM` layer with `input_shape=(LOOK_BACK_WINDOW, num_features)`. Use 50-100 units. `return_sequences` must be `False` as we are predicting a single output value.\n",
      "    *   **Hidden Layer (Optional but Recommended):** Add a `Dense` layer (e.g., 25 units) with `relu` activation.\n",
      "    *   **Output Layer:** Add a `Dense` layer with 1 unit and `linear` activation (for regression).\n",
      "    *   **Compile:** Use the `Adam` optimizer. Set `loss='mean_absolute_error'` (`mae`). Include `metrics=['mean_squared_error', 'mean_absolute_error']`.\n",
      "\n",
      "7.  **Model Training:**\n",
      "    *   Train the model using `X_train` and `y_train`.\n",
      "    *   Use a `validation_split` (e.g., 0.15) from the training data to monitor performance.\n",
      "    *   Set a reasonable number of `epochs` (e.g., 50-100) and `batch_size` (e.g., 32-64).\n",
      "    *   Implement an `EarlyStopping` callback (e.g., `patience=10`, `monitor='val_loss'`) to prevent overfitting and stop training early if validation loss plateaus.\n",
      "\n",
      "8.  **Model Evaluation:**\n",
      "    *   Make predictions on the `X_test` dataset.\n",
      "    *   **Invert Scaling:** This is critical. Both `y_test` (actual values) and the `predictions` are in scaled form. To calculate metrics and plot, they must be converted back to their original scale.\n",
      "        *   Recall that the `StandardScaler` was fitted on all 3 sensor columns. To inverse transform a single column (SensorA), you must create a temporary array of the original `num_features` size (`len(predictions), num_features`), place the scaled `predictions` or `y_test` into the SensorA column (which corresponds to index 0), and then apply `scaler.inverse_transform`. Finally, extract only the SensorA column.\n",
      "    *   Calculate `Mean Absolute Error (MAE)` and `Root Mean Squared Error (RMSE)` between the *original scale* `y_test` and `predictions`. Print these metrics.\n",
      "\n",
      "9.  **Output Generation:**\n",
      "    *   **Directory Creation:** Ensure `figures/` and `models/` directories exist before saving files (`os.makedirs('figures', exist_ok=True)`, `os.makedirs('models', exist_ok=True)`).\n",
      "    *   **Save Results:**\n",
      "        *   Create a JSON file named `results.json` in the current working directory.\n",
      "        *   This file should contain the calculated MAE and RMSE values.\n",
      "        *   Example structure: `{\"MAE\": ..., \"RMSE\": ...}`.\n",
      "    *   **Save Plot:**\n",
      "        *   Generate a plot comparing the *original scale* `y_test` (actual values) and `predictions` for `SensorA` from the test set.\n",
      "        *   Plot them as two distinct lines on the same graph against their time index.\n",
      "        *   Label the x-axis as \"Time Index (Test Set Samples)\" and the y-axis as \"SensorA Value\". Add a legend.\n",
      "        *   Save the plot as `figures/predicted_vs_actual_sensorA.png`.\n",
      "    *   **Save Model:**\n",
      "        *   Save the trained Keras model to `models/lstm_forecaster.h5` using `model.save()`.\n",
      "\n",
      "**Coder's Deliverable:**\n",
      "A single Python script named `run_pipeline.py` that executes all the steps described above.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(AGGREGATOR_SYSTEM_MSG)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "You are a highly skilled Python programmer and Machine Learning Engineer. Your task is to implement a complete time-series forecasting pipeline in a single Python script named `run_pipeline.py`. Follow the detailed instructions below precisely.\n",
      "\n",
      "**Overall Goal:** Predict the value of SensorA 10 minutes into the future, using a 10-minute historical window of readings from all three sensors (SensorA, SensorB, SensorC). This is a univariate time-series forecasting (regression) problem.\n",
      "\n",
      "**Dataset:**\n",
      "*   File: `../sensor_data.csv`\n",
      "*   Content: Minute-level readings (1,440 rows) for 'SensorA', 'SensorB', 'SensorC'.\n",
      "\n",
      "**Model Choice:**\n",
      "*   **Long Short-Term Memory (LSTM) Neural Network**. LSTMs are chosen due to their inherent ability to capture temporal dependencies and patterns in sequential data, making them well-suited for forecasting problems involving \"sensor drift\" and leveraging historical windows.\n",
      "\n",
      "**Detailed Implementation Steps:**\n",
      "\n",
      "1.  **Setup and Data Loading:**\n",
      "    *   Import necessary libraries: `pandas`, `numpy`, `sklearn.preprocessing`, `sklearn.metrics`, `tensorflow.keras.models`, `tensorflow.keras.layers`, `tensorflow.keras.callbacks`, `matplotlib.pyplot`, `json`, `os`.\n",
      "    *   Load the `../sensor_data.csv` file into a pandas DataFrame.\n",
      "    *   The 'time' column is descriptive and not a feature; ensure sensor columns are treated as numeric.\n",
      "\n",
      "2.  **Data Preprocessing:**\n",
      "    *   **Handle Missing Values:**\n",
      "        *   Convert any non-numeric entries (e.g., empty strings) in 'SensorA', 'SensorB', 'SensorC' columns to `np.nan`.\n",
      "        *   Apply **linear interpolation** (`df.interpolate(method='linear', limit_direction='both', axis=0)`) to fill all `np.nan` values across the sensor columns. This method is appropriate for continuous time-series data.\n",
      "    *   **Feature Scaling:**\n",
      "        *   Initialize a `StandardScaler`.\n",
      "        *   **Important:** The scaler will be fitted *only on the training data's sensor columns* later, but prepare for its use.\n",
      "\n",
      "3.  **Data Preparation for LSTM (Windowing):**\n",
      "    *   Define `LOOK_BACK_WINDOW = 10` (minutes) and `FORECAST_HORIZON = 10` (minutes).\n",
      "    *   Create input sequences (`X`) and corresponding target values (`y`) using a sliding window approach:\n",
      "        *   Each `X` sample will be a 3D NumPy array of shape `(LOOK_BACK_WINDOW, num_features)`, representing `num_features=3` sensor readings over 10 minutes. For a given time `t`, the input window will span `[t-9, ..., t]`.\n",
      "        *   Each `y` target will be the single `SensorA` value at `t + FORECAST_HORIZON`. Specifically, if an input window ends at time `t`, the target `y` will be `SensorA`'s value at `t+10`.\n",
      "        *   The total number of samples generated will be `len(df) - LOOK_BACK_WINDOW - FORECAST_HORIZON + 1`.\n",
      "\n",
      "4.  **Train/Test Split:**\n",
      "    *   Perform a **chronological split** of the prepared `X` and `y` data.\n",
      "    *   Allocate 80% of the *earliest* samples for training and the remaining 20% for testing.\n",
      "    *   Example: `train_size = int(len(X) * 0.8)`. `X_train, y_train = X[:train_size], y[:train_size]`. `X_test, y_test = X[train_size:], y[train_size:]`.\n",
      "\n",
      "5.  **Scaling Training and Test Data:**\n",
      "    *   Reshape `X_train` and `X_test` for `StandardScaler` (e.g., `X_train.reshape(-1, num_features)`).\n",
      "    *   Fit the `StandardScaler` *only* on the reshaped `X_train` data (all 3 sensor columns).\n",
      "    *   Transform both the reshaped `X_train` and `X_test` using the fitted scaler.\n",
      "    *   Reshape the scaled `X_train` and `X_test` back to their 3D LSTM input shape `(num_samples, LOOK_BACK_WINDOW, num_features)`.\n",
      "    *   Scale `y_train` and `y_test` separately using the *same scaler* (only fitting on the SensorA part of `X_train` is complex; simplest is to fit the scaler on all sensor data before splitting, but the prompt's `model_restriction` section hints at fitting on train data. Let's adjust: scale *all* sensor data first, then split into X, y, train/test. This simplifies inverse transforming).\n",
      "\n",
      "    **Revised Scaling Strategy:**\n",
      "    *   After missing value imputation, apply `StandardScaler` to the *entire* 'SensorA', 'SensorB', 'SensorC' DataFrame (`df_sensors`).\n",
      "    *   Fit the `StandardScaler` instance on `df_sensors`. This scaler will then be used for inverse transformation.\n",
      "    *   Then, proceed with the windowing (`X`, `y`) and train/test split on this already scaled data.\n",
      "    *   This ensures `y_train` and `y_test` are also correctly scaled by the same scaler that processed the input features, which is crucial for consistent inverse transformation later.\n",
      "\n",
      "6.  **Model Building (LSTM):**\n",
      "    *   Use `tensorflow.keras.Sequential` to build the model.\n",
      "    *   **Input Layer:** `LSTM` layer with `input_shape=(LOOK_BACK_WINDOW, num_features)`. Use 50-100 units. `return_sequences` must be `False` as we are predicting a single output value.\n",
      "    *   **Hidden Layer (Optional but Recommended):** Add a `Dense` layer (e.g., 25 units) with `relu` activation.\n",
      "    *   **Output Layer:** Add a `Dense` layer with 1 unit and `linear` activation (for regression).\n",
      "    *   **Compile:** Use the `Adam` optimizer. Set `loss='mean_absolute_error'` (`mae`). Include `metrics=['mean_squared_error', 'mean_absolute_error']`.\n",
      "\n",
      "7.  **Model Training:**\n",
      "    *   Train the model using `X_train` and `y_train`.\n",
      "    *   Use a `validation_split` (e.g., 0.15) from the training data to monitor performance.\n",
      "    *   Set a reasonable number of `epochs` (e.g., 50-100) and `batch_size` (e.g., 32-64).\n",
      "    *   Implement an `EarlyStopping` callback (e.g., `patience=10`, `monitor='val_loss'`) to prevent overfitting and stop training early if validation loss plateaus.\n",
      "\n",
      "8.  **Model Evaluation:**\n",
      "    *   Make predictions on the `X_test` dataset.\n",
      "    *   **Invert Scaling:** This is critical. Both `y_test` (actual values) and the `predictions` are in scaled form. To calculate metrics and plot, they must be converted back to their original scale.\n",
      "        *   Recall that the `StandardScaler` was fitted on all 3 sensor columns. To inverse transform a single column (SensorA), you must create a temporary array of the original `num_features` size (`len(predictions), num_features`), place the scaled `predictions` or `y_test` into the SensorA column (which corresponds to index 0), and then apply `scaler.inverse_transform`. Finally, extract only the SensorA column.\n",
      "    *   Calculate `Mean Absolute Error (MAE)` and `Root Mean Squared Error (RMSE)` between the *original scale* `y_test` and `predictions`. Print these metrics.\n",
      "\n",
      "9.  **Output Generation:**\n",
      "    *   **Directory Creation:** Ensure `figures/` and `models/` directories exist before saving files (`os.makedirs('figures', exist_ok=True)`, `os.makedirs('models', exist_ok=True)`).\n",
      "    *   **Save Results:**\n",
      "        *   Create a JSON file named `results.json` in the current working directory.\n",
      "        *   This file should contain the calculated MAE and RMSE values.\n",
      "        *   Example structure: `{\"MAE\": ..., \"RMSE\": ...}`.\n",
      "    *   **Save Plot:**\n",
      "        *   Generate a plot comparing the *original scale* `y_test` (actual values) and `predictions` for `SensorA` from the test set.\n",
      "        *   Plot them as two distinct lines on the same graph against their time index.\n",
      "        *   Label the x-axis as \"Time Index (Test Set Samples)\" and the y-axis as \"SensorA Value\". Add a legend.\n",
      "        *   Save the plot as `figures/predicted_vs_actual_sensorA.png`.\n",
      "    *   **Save Model:**\n",
      "        *   Save the trained Keras model to `models/lstm_forecaster.h5` using `model.save()`.\n",
      "\n",
      "**Coder's Deliverable:**\n",
      "A single Python script named `run_pipeline.py` that executes all the steps described above.\n"
     ]
    }
   ],
   "source": [
    "prompt = response.text\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a world-class Python developer and data scientist.\n",
      "Your job is to generate robust, readable, and well-documented Python code...\n",
      "you will follow this instruct :```\n",
      "You are a highly skilled Python programmer and Machine Learning Engineer. Your task is to implement a complete time-series forecasting pipeline in a single Python script named `run_pipeline.py`. Follow the detailed instructions below precisely.\n",
      "\n",
      "**Overall Goal:** Predict the value of SensorA 10 minutes into the future, using a 10-minute historical window of readings from all three sensors (SensorA, SensorB, SensorC). This is a univariate time-series forecasting (regression) problem.\n",
      "\n",
      "**Dataset:**\n",
      "*   File: `../sensor_data.csv`\n",
      "*   Content: Minute-level readings (1,440 rows) for 'SensorA', 'SensorB', 'SensorC'.\n",
      "\n",
      "**Model Choice:**\n",
      "*   **Long Short-Term Memory (LSTM) Neural Network**. LSTMs are chosen due to their inherent ability to capture temporal dependencies and patterns in sequential data, making them well-suited for forecasting problems involving \"sensor drift\" and leveraging historical windows.\n",
      "\n",
      "**Detailed Implementation Steps:**\n",
      "\n",
      "1.  **Setup and Data Loading:**\n",
      "    *   Import necessary libraries: `pandas`, `numpy`, `sklearn.preprocessing`, `sklearn.metrics`, `tensorflow.keras.models`, `tensorflow.keras.layers`, `tensorflow.keras.callbacks`, `matplotlib.pyplot`, `json`, `os`.\n",
      "    *   Load the `../sensor_data.csv` file into a pandas DataFrame.\n",
      "    *   The 'time' column is descriptive and not a feature; ensure sensor columns are treated as numeric.\n",
      "\n",
      "2.  **Data Preprocessing:**\n",
      "    *   **Handle Missing Values:**\n",
      "        *   Convert any non-numeric entries (e.g., empty strings) in 'SensorA', 'SensorB', 'SensorC' columns to `np.nan`.\n",
      "        *   Apply **linear interpolation** (`df.interpolate(method='linear', limit_direction='both', axis=0)`) to fill all `np.nan` values across the sensor columns. This method is appropriate for continuous time-series data.\n",
      "    *   **Feature Scaling:**\n",
      "        *   Initialize a `StandardScaler`.\n",
      "        *   **Important:** The scaler will be fitted *only on the training data's sensor columns* later, but prepare for its use.\n",
      "\n",
      "3.  **Data Preparation for LSTM (Windowing):**\n",
      "    *   Define `LOOK_BACK_WINDOW = 10` (minutes) and `FORECAST_HORIZON = 10` (minutes).\n",
      "    *   Create input sequences (`X`) and corresponding target values (`y`) using a sliding window approach:\n",
      "        *   Each `X` sample will be a 3D NumPy array of shape `(LOOK_BACK_WINDOW, num_features)`, representing `num_features=3` sensor readings over 10 minutes. For a given time `t`, the input window will span `[t-9, ..., t]`.\n",
      "        *   Each `y` target will be the single `SensorA` value at `t + FORECAST_HORIZON`. Specifically, if an input window ends at time `t`, the target `y` will be `SensorA`'s value at `t+10`.\n",
      "        *   The total number of samples generated will be `len(df) - LOOK_BACK_WINDOW - FORECAST_HORIZON + 1`.\n",
      "\n",
      "4.  **Train/Test Split:**\n",
      "    *   Perform a **chronological split** of the prepared `X` and `y` data.\n",
      "    *   Allocate 80% of the *earliest* samples for training and the remaining 20% for testing.\n",
      "    *   Example: `train_size = int(len(X) * 0.8)`. `X_train, y_train = X[:train_size], y[:train_size]`. `X_test, y_test = X[train_size:], y[train_size:]`.\n",
      "\n",
      "5.  **Scaling Training and Test Data:**\n",
      "    *   Reshape `X_train` and `X_test` for `StandardScaler` (e.g., `X_train.reshape(-1, num_features)`).\n",
      "    *   Fit the `StandardScaler` *only* on the reshaped `X_train` data (all 3 sensor columns).\n",
      "    *   Transform both the reshaped `X_train` and `X_test` using the fitted scaler.\n",
      "    *   Reshape the scaled `X_train` and `X_test` back to their 3D LSTM input shape `(num_samples, LOOK_BACK_WINDOW, num_features)`.\n",
      "    *   Scale `y_train` and `y_test` separately using the *same scaler* (only fitting on the SensorA part of `X_train` is complex; simplest is to fit the scaler on all sensor data before splitting, but the prompt's `model_restriction` section hints at fitting on train data. Let's adjust: scale *all* sensor data first, then split into X, y, train/test. This simplifies inverse transforming).\n",
      "\n",
      "    **Revised Scaling Strategy:**\n",
      "    *   After missing value imputation, apply `StandardScaler` to the *entire* 'SensorA', 'SensorB', 'SensorC' DataFrame (`df_sensors`).\n",
      "    *   Fit the `StandardScaler` instance on `df_sensors`. This scaler will then be used for inverse transformation.\n",
      "    *   Then, proceed with the windowing (`X`, `y`) and train/test split on this already scaled data.\n",
      "    *   This ensures `y_train` and `y_test` are also correctly scaled by the same scaler that processed the input features, which is crucial for consistent inverse transformation later.\n",
      "\n",
      "6.  **Model Building (LSTM):**\n",
      "    *   Use `tensorflow.keras.Sequential` to build the model.\n",
      "    *   **Input Layer:** `LSTM` layer with `input_shape=(LOOK_BACK_WINDOW, num_features)`. Use 50-100 units. `return_sequences` must be `False` as we are predicting a single output value.\n",
      "    *   **Hidden Layer (Optional but Recommended):** Add a `Dense` layer (e.g., 25 units) with `relu` activation.\n",
      "    *   **Output Layer:** Add a `Dense` layer with 1 unit and `linear` activation (for regression).\n",
      "    *   **Compile:** Use the `Adam` optimizer. Set `loss='mean_absolute_error'` (`mae`). Include `metrics=['mean_squared_error', 'mean_absolute_error']`.\n",
      "\n",
      "7.  **Model Training:**\n",
      "    *   Train the model using `X_train` and `y_train`.\n",
      "    *   Use a `validation_split` (e.g., 0.15) from the training data to monitor performance.\n",
      "    *   Set a reasonable number of `epochs` (e.g., 50-100) and `batch_size` (e.g., 32-64).\n",
      "    *   Implement an `EarlyStopping` callback (e.g., `patience=10`, `monitor='val_loss'`) to prevent overfitting and stop training early if validation loss plateaus.\n",
      "\n",
      "8.  **Model Evaluation:**\n",
      "    *   Make predictions on the `X_test` dataset.\n",
      "    *   **Invert Scaling:** This is critical. Both `y_test` (actual values) and the `predictions` are in scaled form. To calculate metrics and plot, they must be converted back to their original scale.\n",
      "        *   Recall that the `StandardScaler` was fitted on all 3 sensor columns. To inverse transform a single column (SensorA), you must create a temporary array of the original `num_features` size (`len(predictions), num_features`), place the scaled `predictions` or `y_test` into the SensorA column (which corresponds to index 0), and then apply `scaler.inverse_transform`. Finally, extract only the SensorA column.\n",
      "    *   Calculate `Mean Absolute Error (MAE)` and `Root Mean Squared Error (RMSE)` between the *original scale* `y_test` and `predictions`. Print these metrics.\n",
      "\n",
      "9.  **Output Generation:**\n",
      "    *   **Directory Creation:** Ensure `figures/` and `models/` directories exist before saving files (`os.makedirs('figures', exist_ok=True)`, `os.makedirs('models', exist_ok=True)`).\n",
      "    *   **Save Results:**\n",
      "        *   Create a JSON file named `results.json` in the current working directory.\n",
      "        *   This file should contain the calculated MAE and RMSE values.\n",
      "        *   Example structure: `{\"MAE\": ..., \"RMSE\": ...}`.\n",
      "    *   **Save Plot:**\n",
      "        *   Generate a plot comparing the *original scale* `y_test` (actual values) and `predictions` for `SensorA` from the test set.\n",
      "        *   Plot them as two distinct lines on the same graph against their time index.\n",
      "        *   Label the x-axis as \"Time Index (Test Set Samples)\" and the y-axis as \"SensorA Value\". Add a legend.\n",
      "        *   Save the plot as `figures/predicted_vs_actual_sensorA.png`.\n",
      "    *   **Save Model:**\n",
      "        *   Save the trained Keras model to `models/lstm_forecaster.h5` using `model.save()`.\n",
      "\n",
      "**Coder's Deliverable:**\n",
      "A single Python script named `run_pipeline.py` that executes all the steps described above. for implement code exactly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "import os\n",
    "BONUS_SYSTEM_PROMPT = f\"\"\"\n",
    "You are a world-class Python developer and data scientist.\n",
    "Your job is to generate robust, readable, and well-documented Python code...\n",
    "you will follow this instruct :{prompt} for implement code exactly\n",
    "\"\"\"\n",
    "\n",
    "print(BONUS_SYSTEM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Code written to run_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "responses = model.generate_content(BONUS_SYSTEM_PROMPT)\n",
    "\n",
    "# 4. Trích xuất code từ ```...``` block\n",
    "code_block = responses.text\n",
    "match = re.search(r\"```(?:python)?\\s*(.*?)```\", code_block, re.DOTALL)\n",
    "code = match.group(1).strip() if match else code_block.strip()\n",
    "\n",
    "# 5. Ghi code ra file\n",
    "with open(\"run_pipeline.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "print(\"✅ Code written to run_pipeline.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Title: Sensor Anomaly Detection Using Window-Based Classification
# Experiment description: Train a classifier (e.g., Random Forest, Logistic Regression, or LSTM) on time-windowed sensor data to detect anomalies. The dataset is split into overlapping windows to preserve temporal patterns. The goal is to predict whether a window of sensor readings contains an anomaly using classical machine learning (e.g., scikit-learn) or deep learning (e.g., LSTM). Models are evaluated using validation accuracy, precision, recall, and F1-score.
## Run 0: Baseline
Results: {'evaluation_metrics': {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}, 'model_hyperparameters': {'n_estimators': 100, 'random_state': 42}, 'pipeline_parameters': {'window_size': 60, 'overlap': 30, 'anomaly_perturbation_factor_std': [5, 10], 'data_split_ratios': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}}
Description: Baseline results.

## Run 1: Investigate Window Size (Smaller Window)
Results: {'evaluation_metrics': {'accuracy': 0.9285714285714286, 'precision': 0.9285714285714286, 'recall': 1.0, 'f1_score': 0.9629629629629629}, 'model_hyperparameters': {'n_estimators': 100, 'random_state': 42}, 'pipeline_parameters': {'window_size': 30, 'overlap': 15, 'anomaly_perturbation_factor_std': [5, 10], 'data_split_ratios': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}}
Description: This run explored the impact of a smaller window size (30) and a proportionally reduced overlap (15) on anomaly detection performance. The goal was to see if less temporal context would affect the classifier's ability to identify anomalous windows. Compared to the baseline, there was a slight decrease in accuracy, precision, and F1-score, suggesting that the original window size of 60 might be more suitable for capturing the necessary patterns.

## Run 2: Investigate Overlap (Increased Overlap)
Results: {'evaluation_metrics': {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}, 'model_hyperparameters': {'n_estimators': 100, 'random_state': 42}, 'pipeline_parameters': {'window_size': 60, 'overlap': 45, 'anomaly_perturbation_factor_std': [5, 10], 'data_split_ratios': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}}
Description: This run investigated the effect of increasing the window overlap from 30 to 45, while keeping the window size at the baseline of 60. The objective was to see if providing more overlapping context between windows would improve or maintain performance. The results show perfect scores (accuracy, precision, recall, F1-score all 1.0), matching the baseline. This indicates that increased overlap did not negatively impact the model's ability to detect anomalies on this synthetic dataset.

## Run 3: Investigate n_estimators (Increased Trees for Random Forest)
Results: {'evaluation_metrics': {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}, 'model_hyperparameters': {'n_estimators': 200, 'random_state': 42}, 'pipeline_parameters': {'window_size': 60, 'overlap': 30, 'anomaly_perturbation_factor_std': [5, 10], 'data_split_ratios': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}}
Description: This run explored the effect of increasing the number of estimators (trees) in the Random Forest classifier from 100 to 200, while keeping other parameters at their baseline values. The aim was to see if a more complex model would yield better anomaly detection performance. Similar to the baseline and Run 2, the model achieved perfect scores across all evaluation metrics. This suggests that for the current synthetic dataset, 100 estimators are already sufficient, and increasing them further does not provide additional benefit.

## Run 4: Investigate Anomaly Perturbation Factor (Larger Anomaly Magnitudes)
Results: {'evaluation_metrics': {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}, 'model_hyperparameters': {'n_estimators': 100, 'random_state': 42}, 'pipeline_parameters': {'window_size': 60, 'overlap': 30, 'anomaly_perturbation_factor_std': [10, 15], 'data_split_ratios': {'train': 0.8, 'validation': 0.1, 'test': 0.1}}}
Description: This run examined the impact of increasing the magnitude of synthetic anomalies by changing the `anomaly_perturbation_factor_std` from (5, 10) to (10, 15). The goal was to see if larger, more pronounced anomalies would affect the classifier's detection capabilities. The model continued to achieve perfect scores across all metrics, indicating that even with increased perturbation, the anomalies remain highly distinguishable for the Random Forest classifier within the current data generation and windowing setup.

## Plot Descriptions

The `plot.py` script generates four bar charts, each comparing a specific evaluation metric across all the conducted experiment runs (Baseline, Run 1, Run 2, Run 3, Run 4, and Run 5). These plots provide a visual summary of how changes in windowing parameters, model hyperparameters, and anomaly generation parameters affected the anomaly detection performance.

### 1. Accuracy Comparison
- **Filename:** `plots/accuracy_comparison.png`
- **Description:** This bar chart displays the accuracy score for each experiment run. Accuracy represents the proportion of correctly classified windows (both anomalous and normal) out of the total number of windows. It provides an overall measure of the model's correctness. This plot helps to quickly identify which configurations led to higher overall correct predictions.

### 2. Precision Comparison
- **Filename:** `plots/precision_comparison.png`
- **Description:** This bar chart illustrates the precision score for each experiment run. Precision is the ratio of true positive predictions (correctly identified anomalous windows) to the total number of positive predictions (all windows predicted as anomalous). High precision indicates a low rate of false positives, meaning fewer normal windows are incorrectly flagged as anomalous. This plot is crucial for understanding the model's ability to avoid false alarms.

### 3. Recall Comparison
- **Filename:** `plots/recall_comparison.png`
- **Description:** This bar chart shows the recall score (also known as sensitivity or true positive rate) for each experiment run. Recall is the ratio of true positive predictions (correctly identified anomalous windows) to the total number of actual positive instances (all actual anomalous windows). High recall indicates a low rate of false negatives, meaning the model is effective at finding most of the actual anomalies. This plot is important for assessing the model's ability to detect all relevant anomalies.

### 4. F1-Score Comparison
- **Filename:** `plots/f1_score_comparison.png`
- **Description:** This bar chart presents the F1-score for each experiment run. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is particularly useful when there is an uneven class distribution (e.g., fewer anomalies than normal data points). A higher F1-score indicates a better balance between precision and recall, suggesting a robust model performance in identifying anomalies without excessive false positives or negatives.
